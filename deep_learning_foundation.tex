\documentclass[10pt,mathserif]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag}
\usepackage{pythonhighlight, subcaption, natbib}


\input{preamble}

\mode<presentation>
{
\usetheme{default}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\AtBeginSection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[currentsection,currentsubsection]
	\end{frame}
}

%% begin presentation

\title{\large \bfseries Deep Learning Foundations}

\author{Kris Sankaran\\[3ex] Nepal Winter School in AI}

\date{\today}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

\begin{frame}
  \frametitle{Learning Objectives}
  \begin{itemize}
    \item Understand goals of deep learning, especially representation learning
    \item Logistic Regression to Backpropagation: Learn the modular-components
      view of deep learning
    \item Learn tricks for doing (and debugging) deep learning
    \item How to study deep learning algorithms: toy examples,
      drawing pictures, ...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Flipping Coins}
  \begin{itemize}
  \item Imagine $n$ flips of a coin with probability $\pi$ of coming up heads.
  \item Loglikelihood function (independent bernoulli trials)
    \begin{align*}
      \log p\left(y \vert \pi \right) &= \log \left[\prod_{i = 1}^{n} \pi^{\indic{y_i = 1}}\left(1 - \pi\right)^{y_i = 0}\right] \\
      &= \sum_{i = 1}^{n} \indic{y_i = 1}\log \pi + \indic{y_i = 0}\log\left(1 - \pi\right)
    \end{align*}
    where we use the shorthand $y = \left(y_1, \dots, y_n\right)$.
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/loglikelihood_bernoulli}
    \caption{Logliklihoods over $\pi$ when we see one head and one tail. Seems
      most likely that $\pi \approx 0.5$. \label{fig:loglikelihood_bernoulli} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Flipping Coins}
  \begin{itemize}
  \item Imagine $n$ flips of a coin with probability $\pi$ of coming up heads.
  \item Loglikelihood function (independent bernoulli trials)
    \begin{align*}
      \log p\left(y \vert \pi \right) &= \log \left[\prod_{i = 1}^{n} \pi^{\indic{y_i = 1}}\left(1 - \pi\right)^{y_i = 0}\right] \\
      &= \sum_{i = 1}^{n} \indic{y_i = 1}\log \pi + \indic{y_i = 0}\log\left(1 - \pi\right)
    \end{align*}
    where we use the shorthand $y = \left(y_1, \dots, y_n\right)$.
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/loglikelihood_bernoulli_10}
    \caption{Logliklihoods over $\pi$ when we see 3 heads and 7 tails. Seems
      most likely that $\pi \approx 0.3$. \label{fig:loglikelihood_bernoulli_10} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Flipping Coins}
  \begin{itemize}
  \item Imagine $n$ flips of a coin with probability $\pi$ of coming up heads.
  \item Loglikelihood function (independent bernoulli trials)
    \begin{align*}
      \log p\left(y \vert \pi \right) &= \log \left[\prod_{i = 1}^{n} \pi^{\indic{y_i = 1}}\left(1 - \pi\right)^{y_i = 0}\right] \\
      &= \sum_{i = 1}^{n} \indic{y_i = 1}\log \pi + \indic{y_i = 0}\log\left(1 - \pi\right)
    \end{align*}
    where we use the shorthand $y = \left(y_1, \dots, y_n\right)$.
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/loglikelihood_bernoulli_100}
    \caption{Logliklihoods over $\pi$ when we see 30 heads and 70 tails. Again seems
      most likely that $\pi \approx 0.3$, but everything is scaled by $10
      \times$. \label{fig:loglikelihood_bernoulli_100} }
  \end{figure}
\end{frame}

%% \begin{frame}
%%   \frametitle{Flipping Coins}
%%   \begin{itemize}
%%   \item Imagine $n$ flips of a coin with probability $\pi$ of coming up heads.
%%   \item Entropy function
%%     \begin{align*}
%%       -\Esubarg{p}{\log p\left(x\right)} &= \sum_{i = 1}^{n} \Esubarg{p}{\indic{x_i = 1}}\log\pi + \Esubarg{p}{x_i = 0}\left(1 - \pi\right) \\
%%       &= n \left[\pi\log\pi + \left(1 - \pi\right)\log\left(1 - \pi\right)\right]
%%     \end{align*}
%%   \end{itemize}
%% \end{frame}

\begin{frame}
  \frametitle{Using Input Data}
  \begin{itemize}
  \item What if the probability depended on input data?
  \item Depending on some input $x_i$, we flip coin with probability
    $\pi\left(x_i\right)$ of heads
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/logistic_scatter}
    \caption{Example of a classification problem when $x_i$ are two dimensional,
      heads are green, and tails are purple. \label{fig:logistic_scatter} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Using Input Data}
  \begin{itemize}
  \item What if the probability depended on input data?
  \item Depending on some input $x_i$, we flip coin with probability
    $\pi\left(x_i\right)$ of heads
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/logistic_scatter_plane}
    \caption{Example of a classification problem when $x_i$ are two dimensional,
      heads are green, and tails are purple. \label{fig:logistic_scatter} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Using Input Data}
  \begin{itemize}
  \item What if the probability depended on input data?
  \item Depending on some input $x_i$, we flip coin with probability
    $\pi\left(x_i\right)$ of heads
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/logistic_scatter_nonlinear_points}
    \caption{Example with a nonlinear
      boundary. \label{fig:logistic_nonlinear_scatter_points} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Using Input Data}
  \begin{itemize}
  \item What if the probability depended on input data?
  \item Depending on some input $x_i$, we flip coin with probability
    $\pi\left(x_i\right)$ of heads
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/logistic_scatter_nonlinear}
    \caption{Example with a nonlinear
      boundary. \label{fig:logistic_nonlinear_scatter_points} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Using Input Data}
  \begin{itemize}
  \item What if the probability depended on input data?
  \item Depending on some input $x_i$, we flip coin with probability
    $\pi\left(x_i\right)$ of heads
  \item Can adapt previous loglikelihood
    \begin{align*}
      \log p\left(y \vert x\right) &= \sum_{i = 1}^{n} \indic{y_i = 1}\log \pi\left(x\right) + \indic{y_i = 0}\log\left(1 - \pi\left(x\right)\right)
    \end{align*}
  \item Need to learn $\pi\left(x\right)$, just like we learned $\pi$ before
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Sigmoid Function}
  \begin{itemize}
  \item Consider the collection of functions (one for each $\theta$)
    \begin{align*}
      \pi_{\theta}\left(x\right) &= \frac{1}{1 + \exp{-\theta^{T}x}} \stackrel{\text{defined}}{=}\sigma\left(\theta^{T}x\right)
    \end{align*}
  \end{itemize}
  \begin{figure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_1}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_2}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_3}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_4}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_5}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_6}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_7}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_8}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_9}
    \end{subfigure}
    \begin{subfigure}{.17\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/sigmoid_plot_10}
    \end{subfigure}
    \caption{Example sigmoid functions $\pi_{\theta}$ for random draws of
      $\theta$, when we assume $x$ has an intercept term ($x = \left(1,
      \dots\right)$). Notice that there are no nonlinear boundaries...
      \label{fig:logistic_scatter_nonlinear} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Logistic Regression}
  \begin{itemize}
  \item Logistic Regression $\rightarrow$ Finding good $\theta$'s for this
    likelihood (i.e., the ones that maximize it)
    \begin{align*}
      \log p_{\theta}\left(y \vert x\right) &= \sum_{i = 1}^{n} \indic{y_i = 1}\log \pi_{\theta}\left(x\right) + \indic{y_i = 0}\log\left(1 - \pi_{\theta}\left(x\right)\right)
    \end{align*}
  \item These are the directions like those in the previous figure that do a
    good job separating heads from tails
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Optimization}
  \begin{itemize}
  \item Need to find some $\theta$'s that maximize $\log p_{\theta}\left(y \vert s\right)$
  \item Use gradient descent on $-\log p_{\theta}\left(y \vert x\right)$
  \item How to get the gradients?
  \item Approach 1: Courageously differentiate,
    \begin{align*}
      &\frac{\partial}{\partial \theta}\log p_{\theta}\left(y \vert x\right) \\
      = &\frac{\partial}{\partial \theta} \left[\sum_{i = 1}^{n} y_i \log \sigma\left(\theta^{T}x_{i}\right) + \left(1 - y_i\right)\log\left(1 - \sigma\left(\theta^{T}x_i\right)\right)\right] \\
      = &\frac{\partial}{\partial \theta}\left[\sum_{i = 1}^{n}
        y_i \log\left(\frac{1}{1 + \exp{-\theta^{T}x_i}}\right) +
          \left(1 - y_i\right)\log\left(\frac{\exp{-\theta^{T}x_i}}{1 + \exp{-\theta^{T}x_i}}\right) \right]
    \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Optimization}
  \begin{itemize}
  \item Approach 2: Chain rule magic!
    \begin{align*}
      \log p_{\theta}\left(y_i \vert x_i\right)  &= y_i \log\left(\theta^T x_i\right) + \left(1 - y_i\right) \log\left(1 - \sigma\left(\theta^T x_i\right)\right) \\
      &= \ell\left(\sigma\left(\theta^T x_{i}\right)\right)
    \end{align*}
  \item This is the composition of three relatively simple functions
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/logistic_comp_graph}
  \caption{The loss function is a series of compositions of relatively simple
    functions. This is an example of a computational flow diagram, which is
    often used in the deep learning literature.
    \label{fig:logistic_comp_graph} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Review: The Chain Rule}
  \begin{itemize}
  \item Derivative of composition $\rightarrow$ multiplication of derivatives
    \begin{align*}
      D\left(f \circ g\right)\left(x\right) &= D f\left(g\left(x\right)\right)Dg\left(x\right)
    \end{align*}
  \end{itemize}
 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.75\paperwidth]{figure/chain_rule_2}
   \caption{The chain rule tells us how a small change in $x$ affects the
     downstream output of a composition of functions, in terms of the changes it
     causes within each intermediate function. \label{fig:chain_rule_2} }
 \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Review: The Chain Rule}
  \begin{itemize}
  \item Derivative of composition $\rightarrow$ multiplication of derivatives
    \begin{align*}
      D\left(f^{K} \circ f^{K - 1} \dots \circ f^{1}\right)\left(x\right) &= \left[\prod_{k = 2}^{K} D f^{k}\left(h_{k - 1}\right)\right]D f^1\left(x\right)
    \end{align*}
    where $h_k = f^{k}\left(h_{k - 1}\right)$ is output of previous level (and $h_0 = x$)
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\paperwidth]{figure/chain_rule_k}
    \caption{The chain rule tells us how a small change in $x$ affects the
      downstream output of a composition of functions, in terms of the changes it
      causes within each intermediate function. \label{fig:chain_rule_k} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Chain Rule for Logistic Regression}
  \begin{itemize}
  \item We can now find the derivative of the logistic regression loss much more
    elegantly
  \item Remember that it has these intermediate functions
    \begin{enumerate}
    \item $\ell\left(\sigma\right) = y_i\log\sigma + \left(1 - y_i\right)\log\left(1 - \sigma\right)$
    \item $\sigma\left(z\right) = \frac{1}{1 + \exp{-z}}$
    \item $\theta^{T}x_i$
    \end{enumerate}
  \item Their derivatives are (check this!)
    \begin{enumerate}
    \item $\frac{\partial}{\partial \sigma}\ell\left(\sigma\right) = \frac{y_i}{\sigma} - \frac{1 - y_i}{1 - \sigma}$
    \item $\frac{\partial}{\partial z}\sigma\left(z\right) = -\sigma\left(z\right)\left(1 - \sigma\left(z\right)\right)$
    \item $\nabla_{\theta} \theta^{T} x_i = x_i$
    \end{enumerate}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Chain Rule for Logistic Regression}
  \begin{itemize}
  \item Multiply the derivatives and substitute input from the flow diagram
  \item Gives the (negative) gradient,
    \begin{align*}
      &\left(\frac{y_i}{\sigma\left(\theta^{T}x_i\right)} - \frac{1 - y_i}{1 - \sigma\left(\theta^{T}x_i\right)}\right)\sigma\left(\theta^{T}x_i\right)\left(1 - \sigma\left(\theta^{T}x_i\right)\right)x_i \\
      = &\left[y_i\left(1 - \sigma\left(\theta^{T} x_i\right)\right) - \left(1 - y_i\right)\sigma\left(\theta^T x_i\right)\right]x_i \\
      = &\left(y_i - \pi_{\theta}\left(x_i\right)\right) x_i
    \end{align*}
    recalling that by definition $\pi_{\theta}\left(x_i\right) =
    \sigma\left(\theta^T x_i\right)$
  \item Nice interpretation: change $\theta$ a lot when $y_i$ is far from the
    prediction probability $\pi_{\theta}\left(x_i\right)$
  \end{itemize}
\end{frame}

\section{Representation Learning}

\begin{frame}
  \frametitle{Finding Meaningful Features}
 \begin{itemize}
 \item Want to estimate $p\left(y_i = \text{yak} \vert x_i\right)$
 \item Could use logistic regression if the input $x_i$ were meaningful
 \end{itemize}
 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.7\paperwidth]{figure/image_features_ideal}
   \caption{It would be nice if our images came with qualitative annotation
     about what was in them. \label{fig:image_features_ideal} }
 \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Finding Meaningful Features}
 \begin{itemize}
 \item Want to estimate $p\left(y_i = \text{yak} \vert x_i\right)$
 \item If our $x_i$'s are unstructured, this becomes much more difficult
 \item Logistic regression will fail
 \end{itemize}
 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.7\paperwidth]{figure/image_features_reality}
   \caption{In reality, we can't manually provide all this
     annotation. \label{fig:image_features_reality} }
 \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Finding Meaningful Features}
 \begin{itemize}
 \item If our $x_i$'s are unstructured, this becomes much more difficult
 \item Logistic regression will fail
 \end{itemize}
 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.7\paperwidth]{figure/language_features}
   \caption{Similarly, extracting meaningful structure from raw text would be
     useful in a variety of downstream tasks. \label{fig:language_features} }
 \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Our Goal}
  \begin{itemize}
  \item Try to automatically learn meaningful features
  \item Start with simple features at low layers
  \item Increase feature complexity in a compositional way
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/zf_layer1}
    \caption{Early layers might create features corresponding to edges of
      different orientations. \label{fig:zf_layer1} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Our Goal}
  \begin{itemize}
  \item Try to automatically learn meaningful features
  \item Start with simple features at low layers
  \item Increase feature complexity in a compositional way
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\paperwidth]{figure/zf_layer2}
    \caption{Features increase in complexity as you get deeper in the
      network. \label{fig:zf_layer2} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Our Goal}
  \begin{itemize}
  \item Try to automatically learn meaningful features
  \item Start with simple features at low layers
  \item Increase feature complexity in a compositional way
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/zf_layer4}
    \caption{Eventually we will have meaningful high-level features.
      \label{fig:zf_layer4} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Our Goal}
  \begin{itemize}
  \item Try to automatically learn meaningful features
  \item Start with simple features at low layers
  \item Increase feature complexity in a compositional way
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\paperwidth]{figure/high_layer_text}
    \caption{Similar kinds of complex features can emerge in text applications
      too.
      \label{fig:layer_text} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Mixing \& Stacking Logistic Regressions}
  \begin{itemize}
    \item Simple combinations of simple functions quickly become complex
    \item These complex compositions could capture meaningful features
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\paperwidth]{figure/mixture_logistic_2}
    \caption{Simply mixing two logistic regressions, we can represent
      nonmonotonic functions. \label{fig:mixture_logistic_2} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Mixing \& Stacking Logistic Regressions}
  \begin{itemize}
  \item Simple combinations of simple functions can quickly become complex
  \item These complex compositions could capture meaningful features
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.55\paperwidth]{figure/mixture_logistic_k}
    \caption{Mixing more sigmoids, we can represent even more complicated
      functions. \label{fig:mixture_logistic_k} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Mixing \& Stacking Logistic Regressions}
  \begin{itemize}
  \item New notation: Representation at layer $k$,
    \begin{align*}
      h_{k} = f^k\left(h_{k - 1}; W_k\right) \stackrel{\text{defined}}{=} \sigma\left(W_k h_{k - 1}\right)
    \end{align*}
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.4\paperwidth]{figure/mixture_logistic_k_stacked}
    \caption{Mixing more sigmoids, we can represent even more complicated
      functions. \label{fig:mixture_logistic_k_block} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Mixing \& Stacking Logistic Regressions}
  \begin{itemize}
  \item New notation: Representation at layer $k$,
    \begin{align*}
      h_{k} = f^k\left(h_{k - 1}; W_k\right) \stackrel{\text{defined}}{=} \sigma\left(W_k h_{k - 1}\right)
    \end{align*}
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.4\paperwidth]{figure/mixture_logistic_k_stacked_loss}
    \caption{Mixing more sigmoids, we can represent even more complicated
      functions. \label{fig:mixture_logistic_k_block} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Mixing \& Stacking Logistic Regressions}
  \begin{itemize}
  \item Simple combinations of simple functions can quickly become complex
  \item These complex compositions could capture meaningful features
  \end{itemize}
  \begin{figure}
    \centering
    \includegraphics[width=0.7\paperwidth]{figure/deep_learning_basic}
    \caption{The basic idea of deep learning is to compose nonlinearities in a
      way that encourages discovery of complex intermediate representations.
      \label{fig:deep_learning_basic} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Aside: Activations}
  \begin{itemize}
  \item For nonlinearity, we used sigmoid
  \item Many types of nonlinearities can be used instead
  \item The basic idea of mixing together simple functions remains the same
  \item Most common these days are Rectified Lienar Unites (ReLUs)
    \begin{itemize}
    \item They are easier to optimize (alleviate vanishing gradient problem)
    \end{itemize}
  \end{itemize}
\begin{figure}[ht]
  \centering
    \begin{subfigure}{0.16\paperwidth}
      \centering
      \includegraphics[width=0.16\paperwidth]{figure/activation_examples_1}
    \end{subfigure}
    \begin{subfigure}{.16\paperwidth}
      \centering
      \includegraphics[width=0.16\paperwidth]{figure/activation_examples_2}
    \end{subfigure}
  \caption{\label{fig:activations} Some example nonlinearities from
    \url{https://en.wikipedia.org/wiki/Activation_function}. }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Optimization: Stochastic Gradient Descent}
  \begin{itemize}
  \item Optimize by moving in a direction expected to decrease loss
  \item SGD: Estimated direction of gradient, after looking at small batch of
    data
  \item These days, other methods used too -- Adam, AdaGrad, AdaDelta, ...
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/sgd_example}
  \caption{Each optimization step modifies model parameters to make the fitted
    function closer ot the true (local) minimizer of the loss.
    \label{fig:sgd_example} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Inductive View of Backpropagation}
  \begin{itemize}
  \item For each iteration of SGD, we need to estimate the gradient of the loss
    with respect to the parameters
  \item When you perturb $W_k$ a little, it changes all the upstream $h_k, h_{k +
    1}, \dots, h_{K}$ up to the prediction, which uses that final representation
  \item There is a lot of composition going on here... could become much more
    complicated than logistic regression
  \item Chain rule saves the day again!
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Base Case}
  \begin{itemize}
  \item Loss using final $K^{th}$ layer representation (a single number $h_k$,
    giving probability coin toss is $1$)
    \begin{align*}
      \ell\left(y_i, h_K\right) &= y_i \log h_{K} + \left(1 - y_i\right)\log\left(1 - h_{K}\right)
    \end{align*}
  \item How does the loss change when we perturb $h_{K}$ slightly?
    \begin{align*}
      D \ell\left(h_{K}\right) &= \frac{y_i}{h_K} - \frac{1 - y_i}{1 - h_K}
    \end{align*}
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\paperwidth]{figure/backprop_last_layer}
  \caption{We are considering the derivative of the lost with respect to changes
    in the last layer. \label{fig:backprop_last_layer} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Inductive Hypothesis + Step}
  \begin{itemize}
  \item Hypothesis: Suppose we know how to compute derivatives for all $f_{K},
    f_{K - 1}, \dots, f_{k + 2}$ with respect to their input $h$'s
  \item Step: Then derivative of $\ell$ with respect to upstream $h_k$
    \begin{align*}
      D\left(\ell \circ f^{K}\circ \cdots \circ f^{k + 1}\right)\left(h_{k}\right) &=
      D\ell\left(h_{K}\right) Df^{K}\left(h_{k - 1}\right)\cdots Df^{k + 1}\left(h_{k}\right)
    \end{align*}
  \item All but the last term known by hypothesis
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\paperwidth]{figure/backprop_inductive_step}
  \caption{Assuming we know how to take derivatives of all layers, we can take
    derivatives with respect to the $k^{th}$
    layer. \label{fig:backprop_inductive_step} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Inductive Hypothesis + Step}
  \begin{itemize}
  \item Last term can be found by two-term chain rule, since it is a composition
    of linear and sigmoid transformation: $D\left(\sigma\left(W^{k +
      1}h_{k}\right)\right)$
  \item Therefore, we compute any $D\ell\left(h_k\right)$
    \begin{itemize}
    \item Abuse of notation: $\ell$ is not directly a function of $h_k$.
      Interpret this as change in $\ell$ when you perturb upstream $h_k$
    \end{itemize}
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\paperwidth]{figure/backprop_inductive_step}
  \caption{Assuming we know how to take derivatives of all layers, we can take
    derivatives with respect to the $k^{th}$
    layer. \label{fig:backprop_inductive_step} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Backpropagation: Updating $W_k$'s}
  \begin{itemize}
  \item Know how loss changes when intermediate layers $h_k$ are perturbed
  \item But for SGD, we need derivatives with respect to $W_k$'s
  \item Again abusing notation, use known derivatives w.r.t. inputs
    \begin{align*}
      D\ell\left(W_k\right) &= D \left(\ell \circ f^{k + 1}\right)\left(W_k\right) \\
        &= D\ell\left(h_{k + 1}\right)Df_{k + 1}\left(W_k\right)
    \end{align*}
  \item Derivatives of $f_{k + 1}$ with respect to weights is just like logistic
    regression
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/backprop_inductive_w}
  \caption{To get derivatives with respect to weights, multiply by one more edge
    in the computational flow graph. \label{fig:backprop_inductive_w} }
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Modularity}
  \begin{itemize}
  \item Gradients make use only of \textit{local} information
  \item Can propose arbitrary new layers, as long as you can define gradient of
    output with respect to input
  \item Has let people experiment widely in literature,
    \begin{itemize}
    \item Convolution layers \citep{lecun1995convolutional}
    \item Attention mechanisms \citep{olah2016attention}
    \item LSTM cells \citep{schmidhuber1997long}
    \item Residual layers \citep{he2016deep}
    \item ...
    \end{itemize}
  \item And all sorts of mixing and matching between these
  \end{itemize}
\end{frame}

%% \section{Types of Layers}

%% \begin{frame}
%%   \frametitle{Fully Connected}
%% \begin{figure}
%%   \centering
%%   \includegraphics[width=0.7\paperwidth]{figures/fully_connected_vis}
%%   \caption{All the outputs from one layer can affect all the inputs to the next
%%     one. \label{fig:fully_connected}}
%% \end{figure}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Fully Connected (Math)}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Convolution + Pooling}
%%   \begin{figure}
%%     \centering
%%     \includegraphics[width=0.7\paperwidth]{figures/convolutional_vis}
%%     \caption{Convolution compares small patches against prespecified
%%       ``filters.'' Since it uses the same filters for all patches, the number of
%%       parameters is greatly reduced.
%%       \label{fig:convolutional_vis}}
%%   \end{figure}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Residual Layer}
%%   \begin{figure}
%%       \centering
%%       \includegraphics[width=0.7\paperwidth]{figure/resnet_vis}
%%       \caption{Residual Networks use layers to incrementally
%%         add complexity, by directly copying the previous values and only
%%         learning minor modifications on top of that. \label{fig:resnet_vis} }
%%   \end{figure}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Residual Layer (Math)}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Attention Mechanisms}
%% \end{frame}

%% \begin{frame}
%%   \frametitle{Gated Recurrent Units}
%% \end{frame}

\section{Debugging Deep Learning}

\begin{frame}
  \frametitle{Hyperparameter Tuning}
  \begin{itemize}
  \item Deep learning models can be hard to train
  \item Depending on various hyperparameters,
    \begin{itemize}
    \item Sequence of (per-layer) learning rates
    \item Weight initialization
    \item Type, number, and width of each layer
    \item Activation functions
    \end{itemize}
    the model may or may not train properly
  \end{itemize}
\begin{figure}[ht]
  \centering
    \begin{subfigure}{.2\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/learning_rate_effect}
    \end{subfigure}
    \begin{subfigure}{.2\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/weight_initialization}
    \end{subfigure}
    \begin{subfigure}{.2\paperwidth}
      \centering
      \includegraphics[width=0.17\paperwidth]{figure/architecture_choice}
    \end{subfigure}
  \caption{Three types of hyperparameters that can affect training: Learning
    rates, weight initialization, and architecture
    design. \label{fig:hyperparameters} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Training Analysis}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\paperwidth]{figure/bias_variance}
    \caption{Typical strategy is to try to first overfit the model, then
      regularize to ensure validation performance. \label{fig:bias_variance} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Training Analysis}
  \begin{itemize}
  \item It can be helpful to collect statistics
    \begin{itemize}
    \item Gradients
    \item Activation values
    \end{itemize}
  \item Save your model at checkpoints! Helps both debugging and restarting
    training
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \begin{subfigure}{.5\paperwidth}
      \centering
      \includegraphics[width=0.4\paperwidth]{figure/activation_evolution}
    \end{subfigure}
    \begin{subfigure}{.5\paperwidth}
      \centering
      \includegraphics[width=0.4\paperwidth]{figure/backprop_histogram}
    \end{subfigure}
    \caption{Detective work to understand failures in training, from
      \citep{glorot2010understanding}.\label{fig:detective_work} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Philosophy for Learning Deep Learning}
  \begin{itemize}
  \item Don't ignore history!
    \begin{itemize}
    \item No need to keep up with every retweeted paper
    \item There were many creative people long before deep learning
    \end{itemize}
  \item Whenever you encounter a new algorithm or formula,
    \begin{itemize}
    \item Sketch a toy example (one-dimensional, binary instead of multiclass,
      discrete instead of continuous, ...)
    \item Draw picture of a toy example
    \item Try to plug in simple values (0, 1, $\pm \infty$ are good choices)
    \end{itemize}
  \item Genius is overrated
    \begin{itemize}
    \item First of all, there's stackoverflow
    \item No amount of natural talent can make up for dedicated effort
    \end{itemize}
  \end{itemize}
\end{frame}

\bibliography{refs}
\end{document}
