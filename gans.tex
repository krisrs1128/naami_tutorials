\documentclass[10pt,mathserif]{beamer}

\input{preamble.tex}
\usepackage{graphicx,amsmath,amssymb,natbib}

%% \input defs.tex

%% formatting
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\newcommand\footlineon{
  \setbeamertemplate{footline} {
    \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,leftskip=.8cm,rightskip=.6cm]{structure}
      \footnotesize \insertsection
      \hfill
      {\insertframenumber}
    \end{beamercolorbox}
    \vskip 0.45cm
  }\begin{figure}[ht]
  \centering
\end{figure}

}
\footlineon
%% begin presentation

\title{\large \bfseries Generative Adversarial Networks}

\author{Kris Sankaran\\[3ex]
Nepal Winter School in AI}

\date{\today}

\begin{document}

\frame{
  \thispagestyle{empty}
  \titlepage
}

\section{Introduction}

\begin{frame}
  \frametitle{Learning Objectives}
 \begin{itemize}
  \item Understand basic adversarial setup
  \item Mathematical perspectives to help navigate sea of proposed variants
  \item Get a flavor of some real-world applications
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{First thing that comes to mind?}
\begin{itemize}
\item GANs are popular ways to generate images
\end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/gan_example}
  \caption{Probably the most famous GAN samples of all
    time. \citep{radford2015unsupervised} \label{fig:gan_example} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Or maybe this?}
\begin{itemize}
\item GANs are popular ways to generate images
\end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.3\paperwidth]{figure/gan_portrait.jpg}
  \caption{Probably the most expensive GAN sample of all
    time. \label{fig:gan_portrait} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{GANs as Sampling Algorithms}
  \begin{itemize}
  \item Ignore the hype
  \item It's better to think of GANs as learning how to sample from probability
    densities $p\left(x\right)$
    \begin{itemize}
    \item For $32 \times 32$ images, $p\left(x\right)$ is a distribution in
      1024-dimensional space
    \end{itemize}
  \item What's exciting is that they are \textit{implicit} probability models
    \begin{itemize}
    \item Distribution defined implicitly by sampling mechanism
    \end{itemize} 
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/gan_density}
    \caption{We may not be able to write simple formulas for complicated
      densities, but we may be able to write computer programs that sample from
      them.\label{fig:gan_density} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Images and Probability}
\begin{itemize}
\item It's common in the GANs literature to show interpolations between
  generated images
\end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/gan_image_interpolations}
  \caption{Example interpolations between LSUN images in
    \citep{radford2015unsupervised}. This was a convincing result at the time,
    because it showed that the samples weren't simply memorized training
    data. \label{fig:gan_interpolations} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Images and Probability}
\begin{itemize}
  \item Geometrically, these interpolated images are points along a curve in
    image space
\end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.3\paperwidth]{figure/gan_interpolations}
  \caption{The green points would each be an image, and the red line represents
    a sequence of gradually evolving samples, which should look like real
    images, if they live in a region of high probability.
    \label{fig:gan_interpolations_density} }
\end{figure}
\end{frame}

\section{Formulation}

\begin{frame}
  \frametitle{Formulation}
 \begin{itemize}
 \item How to transform (say, uniform) noise $z$ into arbitrary distributions?
 \item Idea: Use a neural network $G$
 \item $z \rightarrow G\left(z\right)$
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.52\paperwidth]{figure/sample_transformation}
  \caption{By passing samples from a probability distribution through a
    function, you get another probability distribution. $y$-values where the
    function is flatter will get more of the original mass, resulting in peaks
    in the transformed distribution. \label{fig:sample_transformation} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Formulation}
 \begin{itemize}
 \item How to learn a transformation to match observed data?
 \item Idea: Use another neural network! Call it $D$.
 \item This is pretty different from usual likelihood-based views
   \begin{itemize}
   \item (there are connections though)
   \end{itemize}
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/discriminator_transformation}
  \caption{\label{fig:discriminator_transformation} }
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Objective Function}
 \begin{itemize}
   \item How should $D$ guide these updates?
   \item Objective function
     \begin{align*}
       \min_{G}\max_{D} V\left(D, G\right) = \Esubarg{p_{\text{data}}}{D\left(x\right)} + \Esubarg{p\left(z\right)}{\log\left(1 - D\left(G\left(z\right)\right)\right)}
     \end{align*}
   \item Alternate gradient steps, first update $D$, then $G$, then $D$, ...
\begin{itemize}
\item $D$ is encouraged to be large on $x$ and small on $G\left(z\right)$, so
  that the objective gets large
\item $G$ is encouraged to make $D$ assign values near 1 to samples $G\left(z\right)$, so that the objective gets small
\end{itemize}
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/gan_objectives}
  \caption{The green line is $\log\left(x\right)$ and the purple is $\log\left(1
    - x\right)$, on the range $\left[0, 1\right]$ of possible
    probabilities. \label{fig:gan_objectives} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{GAN Lab}
 \begin{itemize}
   \url{https://poloclub.github.io/ganlab/}
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/ganlab}
  \caption{A visualization of the whole process, on a few different toy
    datasets. \label{fig:ganlab} }
\end{figure}
\end{frame}

\section{Perspectives}
\label{sec:perspectives}

\begin{frame}
  \frametitle{Training difficulties}
 \begin{itemize}
 \item GANs have a bad reputation for being finnicky to train
 \item If the discriminator is ``too strong,'' it might not communicate useful
   information
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/alternative_critic}
  \caption{We would like a critic that tells the generator when it's getting
    closer, even when it's obviously different from the
    target. \label{fig:alternative_critic} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Wasserstein GAN}
 \begin{itemize}
 \item Formalize this idea using distances $d\left(\P_1, \P_2\right)$ between
   probability distributions
 \item Distances between Gaussians $\Gsn\left(\mu_1, 1\right)$ and
   $\Gsn\left(\mu_2, 1\right)$.
\begin{itemize}
  \item KL-divergence: $1 + \frac{1}{2} \left(\mu_2 - \mu_1\right)^2$
  \item Wasserstein distance:
\end{itemize}
\item Wasserstein distance tells us something even when they are far away!
\item Effect is even more pronounced in very-high dimensions
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Wasserstein Objective}
  \begin{itemize}
  \item Wasserstein distance between data $X$ and generated $\tilde{X}$
    distributions is defined by
    \begin{align*}
      d\left(\P_X, \P_\tilde{X}\right) &= \max_{\|f\|_{L} \leq 1} \Esubarg{X}{f\left(x\right)} - \Esubarg{\tilde{X}}{f\left(\tilde{x}\right)}
    \end{align*}
    where $\|f\|_{L} \leq 1$ is the set of functions whose slopes are always less
    than 1.
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\paperwidth]{figure/wasserstein_tests}
    \caption{You can interpret the objective as computing differences in means
      after passing samples through a variety of (not too wild) test
      functions.\label{fig:wasserstein_tests} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Wasserstein Training}
  \begin{itemize}
  \item In terms of the transformed noise $G\left(z\right)$, this looks like
    \begin{align*}
      d\left(\P_{X}, \P_{\tilde{X}}\right) &= \max_{\|f\| \leq 1} \Esubarg{X}{f\left(x\right)} - \Esubarg{Z}{f\left(G\left(z\right)\right)}
    \end{align*}
  \item Finding the optimal $f$ for any particular choice of $G$ is complicated...
  \item Alternatively do gradient updates
  \begin{itemize}
  \item Critic $f$ inches towards its maximizing value of $d\left(\P_{X},
    \P_{\tilde{X}}\right)$.
  \item Generator $G$ descends towards smaller wasserstein distances
  \item Constraint $\|f\|_{L} \leq 1$ maintained either by clipping or
    regularization
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Implicit Learning}
  \begin{itemize}
  \item Alternative perspective: GANs as likelihood-free inference / implicit
    learning
  \item Remember generative mechanism,
    \begin{align*}
      z &\sim q\left(z\right) \\
      x \vert z &:= G_{\theta}\left(z\right)
    \end{align*}
  \item Marginal density $q_{\theta}\left(x\right)$ of data generated by this
    process is not analytically available
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Estimating Density Ratios}
 \begin{itemize}
 \item Let $p^{\ast}\left(x\right)$ be the true data generating density
 \item Let $y$ be an indicator of true vs. simulated data
   \begin{itemize}
   \item $p\left(x \vert y = 1\right) = p^{\ast}\left(x\right)$
   \item $p\left(x \vert y = 0\right) = q_{\theta}\left(x\right)$
   \end{itemize}
 \item We can't evaluate either $p^\ast$ or $q_{\theta}$ directly, but by Bayes' rule,
   \begin{align*}
     \frac{p^{\ast}\left(x\right)}{q_{\theta}\left(x\right)} &= \frac{p\left(y = 1 \vert x\right)p\left(x\right)}{p\left(y = 1\right)} \frac{p\left(y = 0\right)}{p\left(y = 0 \vert x\right)p\left(x\right)} \\
     &= \frac{p\left(y = 1 \vert x\right)}{p\left(y = 0 \vert x\right)} \frac{1 - \pi}{\pi}
   \end{align*}
   where $\pi = p\left(y = 1\right)$.
   \item We just need a classifier $D_{\varphi}\left(x\right)$ to tell difference between
     truth and simulated
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Learning}
 \begin{itemize}
 \item Lots of options: just specify what you are trying to do (ratio matching,
   density difference) and how you are going to get there (cross-entropy,
   f-divergence, ...)
 \item For example, if you try estimating density ratios using Bernoulli loss,
   \begin{align*}
     &\Esubarg{p\left(x \vert y\right)p\left(y\right)}{y \log D_{\varphi}\left(x\right) + \left(1 - y\right)\log \left(1 - D_{\varphi}\left(x\right)\right)} \\
      = & \pi \Esubarg{p\left(x \vert y = 1\right)}{\log D_{\varphi}\left(x\right)} + \left(1 - \pi\right)\Esubarg{p\left(x \vert y = 0\right)}{\log \left(1 - D_{\varphi}\left(x\right)\right)} \\
     = & \pi \Esubarg{p^{\ast}\left(x\right)}{\log D_{\varphi}\left(x\right)} + \left(1 - \pi\right)\Esubarg{q_{\theta}\left(x\right)}{\log \left(1 - D_{\varphi}\left(x\right)\right)}
   \end{align*}
  which is exactly the usual loss for GANs, if we set $\pi = \frac{1}{2}$.
 \end{itemize}
\end{frame}

\section{Applications}
\label{sec:applications}

\begin{frame}
  \frametitle{Applications: Image-to-Image translation}
\end{frame}

\begin{frame}
  \frametitle{Applications: Inpainting}
\end{frame}

\begin{frame}
  \frametitle{Applications: Super-Resolution}
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
  \begin{itemize}
    \item GANs are generic unsupervised learning procedure that don't require
      explicit probability densities
    \item They learn to transform simple distributions into those that resemble training data
    \item They can be used in many computer vision problems
  \end{itemize}
\end{frame}

\bibliography{refs}
\end{document}
