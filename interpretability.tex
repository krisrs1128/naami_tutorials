\documentclass[10pt,mathserif]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag}

\input{preamble}

%% formatting

\mode<presentation>
{
\usetheme{default}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\AtBeginSection[]
{
	\begin{frame}<beamer>
		\frametitle{Outline}
		\tableofcontents[currentsection,currentsubsection]
	\end{frame}
}

%% begin presentation

\title{\large \bfseries Interpretability in Machine Learning}

\author{Kris Sankaran\\[3ex]
Nepal Winter School in AI}

\date{\today}

\begin{document}

\frame{
  \thispagestyle{empty}
  \titlepage
}

\section{Introduction}

\begin{frame}
  \frametitle{Learning Objectives}
 \begin{itemize}
   \item Realize that deep learning models aren't necessarily black boxes
   \item Distinguish between types of interpretability studied in literature
   \item Understand foundational distillation and perturbation-based methods
 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{What is interpretability?}
  \begin{itemize}
  \item ``Ability to explain or present in
    understandable terms to a human.''
  \item I would add: ability to predict (by hand) the model's behavior under
    different interventions
    \begin{itemize}
      \item Do I get the same prediction if I slightly change neuron 132 in
        layer 3?
      \item What if I deploy my self-driving cars in snowy Montreal, after
        training in sunny Palo Alto?
      \item What if change the subject's race in data point $x_i$?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{General Approaches}
  \begin{itemize}
  \item Resort to a more easily interpretable model
    \begin{itemize}
    \item Distillation: Compress complex model into simpler, more interpretable
      one
    \item Design: Create new model classes competitive with Deep Learning, but
      easier to interpret
    \end{itemize}
  \item Quantify effect of perturbations
    \begin{itemize}
    \item Saliency maps: Pixel-level feature importance
    \item Influence Functions: Importance of individual training examples
    \item Concept Activation: Sensitivity to user-defined concepts
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Linear Models}
\begin{itemize}
  \item In some ways, a gold standard
    \begin{itemize}
    \item Sign and size of $\beta_j$ is meaningful
    \item Effects of changes in $x_j$ easy to predict (even when extrapolating!)
    \end{itemize}
  \item But even here, can be issues
    \begin{itemize}
    \item True effect can be null, but still see nonzero $\beta_j$
    \item Correlated input lead to unstable coefficients
    \item Outliers lead to counterintuitive behavior
    \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Decision Trees}
\begin{itemize}
  \item In some ways, a gold standard
    \begin{itemize}
    \item Can trace ``decision-making'' process
    \item Effects of changes in $x_j$ easy to predict (even when extrapolating!)
    \end{itemize}
  \item But even here, can be issues
    \begin{itemize}
    \item Paths can get complicated for even moderately deep trees
    \item Correlated input lead to unstable splitting patterns
    \end{itemize}
\end{itemize}
\end{frame}


\section{Distillations}

\begin{frame}
  \frametitle{Distillation}
  \begin{itemize}
  \item Intuition: Large models do well because they have better search
    strategies, but learned decision boundaries can be approximated by simpler
    model classes
  \item Strategy: Train a complicated teacher model, and ``distill'' it into a
    simpler student model
  \item (Besides interpretability, often useful for model compression)
  \end{itemize}
  \begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\paperwidth]{figure/distillation_overview}
  \caption{Distilling a complex model to a simple model class often works better
    than attempting to directly learn within the simple model
    class. \label{fig:distillation_overview} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Distillation by Trees}
  \begin{itemize}
  \item Proposal: Approximate teacher logits by class of soft decision trees
    \begin{itemize}
    \item Branching probabilities at node $i$: $\sigma\left(x^{T}w_i +
      b_i\right)$
    \item Internal nodes still learn filters $w_i$
    \item But now can inspect paths for each decision
    \end{itemize}
  \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\paperwidth]{figure/connect4}
  \caption{It's possible to inpsect learned features at decision nodes, as in
    this Connect 4 example. \label{fig:connect4} }
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Additive Models}
  \begin{itemize}
  \item Approximate regression / decision surface by additive model with
    interactions
    \begin{itemize}
    \item $F\left(x\right) = b_0 + \sum h_{j} \left(x_j\right) + \sum_{j \neq k} h_{jk}\left(x_{j}, x_{k}\right) + \dots$
    \end{itemize}
  \item Can directly inspect changes in predictions when changing values of
    specific features
  \item Good for tabular data
  \item Bad when features need to be learned from raw inputs
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Additive Models}
 \begin{figure}[ht]
   \centering
   \includegraphics[width=0.7\paperwidth]{figure/additive}
   \caption{Example first-order additive features learned by
     distillation. \label{fig:additive} }
 \end{figure}
\end{frame}

\section{Perturbations}

\begin{frame}
  \frametitle{Perturbations}
  \begin{itemize}
  \item Intentional perturbation can be illuminating
  \item What happens when you change...
    \begin{itemize}
    \item Raw pixel values?
    \item Presence of training example?
    \item Layer activation values?
    \end{itemize}
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Saliency Maps}
  \begin{itemize}
  \item For a given image, how would class predictions change if you manipulated
    a pixel?
  \item Simplest version: $\nabla_{x} f_{\theta}\left(x\right)$ change in class
    given infinitesimal changes in inputs
  \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\paperwidth]{figure/saliency}
  \caption{Example saliency maps. Smooth-grad just averages the gradient map
    over many noisy versions of input image.\label{fig:saliency} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Cautionary advice...}
 \begin{itemize}
 \item While simple to implement, use cases aren't entirely clear
 \item Still have to inspect one example at a time
 \item Many proposals don't pass \textit{sanity checks}
 \end{itemize} 
\end{frame}

\subsection{Influence Functions}

\begin{frame}
  \frametitle{Influence Functions}
  \begin{itemize}
  \item Classical notion from statistics: Influential datapoints
  \item Measures sensitivity of fit to removal of points
  \end{itemize} 
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.55\paperwidth]{figure/classical_influence}
    \caption{The difference between fits when you include vs. remove a point is
      called that point's influence.\label{fig:classical_influence} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Contributions}
  \begin{itemize}
  \item Change in loss at (potentially new point) $z$ when $\eps$-upweight
    training pair $z_i = \left(x_i, y_i\right)$
    \begin{align*}
    I\left(z, z_i\right) = -\nabla_{\theta} \ell\left(z, \hat{\theta}\right)^{T} H^{-1}_{\hat{\theta}} \nabla_{\theta} \ell\left(z_i, \hat{\theta}\right)
    \end{align*}
  \item Alignment between gradients of loss at new and old points, after
    adjusting for local curvature 
    \begin{itemize}
    \item More alignment $\rightarrow$ larger decrease in loss
    \end{itemize}
  \item Applied of SGD-able approximations to Hessian $H^{-1}_{\hat{\theta}}$
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Applications}
  \begin{itemize}
  \item Identifying most influential points for specific test cases
  \item Identifying mislabled points in training data
  \item Not restricted to deep learning!
  \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.45\paperwidth]{figure/influential}
  \caption{Example influential observations from \citep{koh2017understanding}\label{fig:influential}
  }
\end{figure}
\end{frame}

\subsection{Concept Activation Vectors}

\begin{frame}
  \frametitle{Concept Activation Vectors}
  \begin{itemize}
  \item What if you had a specific ``concept'' that you wanted to test model
    sensitivity to?
    \begin{itemize}
    \item Importance of anemone to predicting clownfish?
    \end{itemize}
  \item Need workable definition of a concept...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Formulation}
  \begin{itemize}
  \item Idea: Have the user provide concepts!
  \item Study how concept affects feature activations, one layer at a time
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{User concepts}
  \begin{itemize}
  \item User creates set of ``concept'' samples (or chooses them from a
    database, using image tags, say)
  \item Create activations $f_{l}$ at layer $l$ for both concept and random
    nonconcept examples
  \item Define concept activation $v_{C}^l$ based on learned linear decision
    boundary
  \end{itemize} 
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\paperwidth]{figure/cav_space}
    \caption{The CAV vector is defined as the direction that separates samples
      representative of a concept (like stripes) and unrelated random samples.
      \label{fig:cav_space} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Effect of Concepts}
  \begin{itemize}
  \item See how logit $h_{lk}$ changes when nudging layer $l$ activations
    $f_{l}\left(x\right)$ for example $x$ towards concept $C$
    \begin{align*}
      S_{k, C, l}\left(x\right) &= \lim_{\eps \downarrow 0} \frac{h_{lk}\left(f_{l}\left(x\right) + \eps v_{C}^l\right) - h_{lk}\left(f_l\left(x\right)\right)}{\eps}
    \end{align*} 
  \item Averaging this over examples $x$ from class of interest
  \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/cav_derivs}
  \caption{The CAV score for a class is the amount the logit for that class
    changes in response to perturbing its examples in the CAV
    direction. \label{fig:cav_derivs} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Applications}
  \begin{itemize}
  \item Find examples within a class most and least similar to specified concept
    \begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\paperwidth]{figure/cav_app3}
    \end{figure}
  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Applications}
  \begin{itemize}
  \item Quantify potential sources of bias
    \begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\paperwidth]{figure/cav_app2}
    \end{figure}
  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Applications}
  \begin{itemize}
  \item Measuring consistency with codified medical practice
    \begin{figure}[ht]
      \centering
      \includegraphics[width=0.7\paperwidth]{figure/cav_app1}
    \end{figure}
  \end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Conclusion}
\begin{itemize}
\item There is no consensus on what makes an algorithm interpretable -- when
  reading a paper, try to clarify the working definition
\item As complicated as it can be to interpret models... still usually simpler
  than interpreting people
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{}
 Tnank you for your attention, and good luck exploring deep learning!

 Remember to enjoy the journey, you want to keep up it for years to come.
\begin{figure}
  \centering
  \includegraphics[width=0.4\paperwidth]{figure/lyrics}
  \caption{Some deep learning poetry, courtesy of the Mila slack
    channel.\label{fig:label} }
\end{figure}

\end{frame}

\bibliography{refs}
\end{document}
