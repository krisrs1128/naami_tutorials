\documentclass[10pt,mathserif]{beamer}

\usepackage{graphicx,amsmath,amssymb,tikz,psfrag}
\usepackage{pythonhighlight}

\input{preamble}

%% formatting

\mode<presentation>
{
\usetheme{default}
}
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\newcommand\footlineon{
  \setbeamertemplate{footline} {
    \begin{beamercolorbox}[ht=2.5ex,dp=1.125ex,leftskip=.8cm,rightskip=.6cm]{structure}
      \footnotesize \insertsection
      \hfill
      {\insertframenumber}
    \end{beamercolorbox}
    \vskip 0.45cm
  }
}
\footlineon

\AtBeginSection[] 
{ 
	\begin{frame}<beamer> 
		\frametitle{Outline} 
		\tableofcontents[currentsection,currentsubsection] 
	\end{frame} 
} 

%% begin presentation

\title{\large \bfseries Bayesian Deep Learning}

\author{Kris Sankaran\\[3ex] Mila}

\date{\today}

\begin{document}
\maketitle

\section{Introduction}
\label{sec:introduction}

\begin{frame}
  \frametitle{Learning Objectives}
  \begin{itemize}
  \item Trace historical development of Bayesian ML, from Variational Inference
    to Bayesian Deep Learning
  \item Add some more models and algorithm to personal catalog of examples
  \item Understand underlying math and code for example algorithms
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Papers from 1990}
  \begin{itemize}
  \item Gelfand and Smith: Renaissance in Bayesian Inference
  \item LeCun et. al.: Early proof of representation learning through neural networks
  \item Common: Use computation to automate tediour, expert-labor intensive processes
  \item Tension: The ``two cultures''... prediction or inference?
  \end{itemize}
  \begin{figure}
    \subfigure{\includegraphics[width=5cm]{figure/lecun}}
    \subfigure{\includegraphics[width=5cm]{figure/gelfand}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Reconciliation}
  \begin{itemize}
  \item We need both inference and prediction for systems that \textit{sense}
    and \textit{act} in the real world
    \begin{itemize}
    \item Need systems running in real time, interfacing with the world
    \item Probabilistic descriptions give us much more to base our decisions off of
    \item It would be nice if everything were automatic, or at least modular
    \end{itemize}
  \item Bayesian deep learning: modular feature learning with uncertainty
  \end{itemize} 
  \begin{figure}
    \subfigure{\includegraphics[width=3cm]{figure/clock}}
    \subfigure{\includegraphics[width=5cm]{figure/beeswarm}}
    \subfigure{\includegraphics[width=5cm]{figure/robot}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Machine Learning checklist}
  Questions to ask every algorithm you meet.
 \begin{itemize}
 \item What is the model class?
 \item What is the evaluation criterion?
 \item What is the search strategy?
 \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Machine Learning checklist}
  In a Bayesian context, should also ask,
  \begin{itemize}
  \item What is the generative mechanism? 
  \item What is the inference criterion?
  \item What is the algorithm?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Classical Bayes}
  \begin{itemize}
  \item Bayes' Rule,
    \begin{align*}
      p\left(\theta \vert x\right) &= \frac{p\left(x \vert \theta\right)p\left(\theta\right)}{p\left(x\right)}
    \end{align*}
  \item Adapt your belief about $\theta$ after seeing $x$
  \item Specify likelihood $p\left(x \vert \theta\right)$ and prior $p\left(\theta\right)$
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Example: Beta-Binomial}
  \begin{itemize}
  \item Task: Identify potential bias in a coin
  \item Model specification,
    \begin{itemize}
    \item Prior: $p \sim \Bet\left(a_0, b_0\right)$
    \item Likelihood: $x \vert p \sim \Bin\left(n, p\right)$
    \end{itemize}
  \item Posterior is still a beta
  \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/betabinomial}
  \caption{\label{fig:betabinomial} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Beta-Binomial in Code}
  \begin{itemize}
  \item We can avoid doing any math by using the \texttt{pyro} package
  \item This is good practice for when it's impossible to do that math
  \item Hinges on the notion of a guide function
  \end{itemize} 
  \begin{python}
    # define the hyperparameters that control the beta prior
    alpha0 = torch.tensor(10.0)
    beta0 = torch.tensor(10.0)

    # sample f from the beta prior
    p = pyro.sample("heads_\prob", dist.Beta(alpha0, beta0))
  \end{python}
  \begin{figure}[ht]
    \centering
    \includegraphics[options]{figure/pyro_model}
    \caption{\label{fig:pyro_model} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Beta-Binomial in Code}
  \begin{itemize}
  \item We can avoid doing any math by using the \texttt{pyro} package
  \item This is good practice for when it's impossible to do that math
  \item Hinges on the notion of a guide function
  \end{itemize} 
  \begin{figure}[ht]
    \centering
    \includegraphics[options]{figure/pyro_inference}
    \caption{\label{fig:pyro_model} }
\end{figure}
\end{frame}

\section{Latent Variable Models}
\label{sec:latent_variable_models}

\begin{frame}
  \frametitle{Mixture Models}
  \begin{itemize}
  \item Sometimes, critical parts of data generating mechanisms are unobserved
    \item Need posterior inference over both mixture parameters $\mu_k,
      \Sigma_k$ as well as assignments $z_i$
  \end{itemize}  
  \begin{align*}
    \mu_{k} &\sim \Gsn\left(0, \Sigma_{0}\right) \\
    z_i &\sim \Cat\left(z_i \vert p\right) \\
    x_i \vert z_i = k &\sim \Gsn\left(x_i \vert \mu_{k}, \Sigma_k\right)
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Other Examples}
  Many widely-used models depend on being able to work with latent variables,
  \begin{itemize}
  \item Exponential Family PCA
  \item Hidden Markov Models
  \item Latent Dirichlet Allocation
  \end{itemize} 
\end{frame}

\begin{frame}[]
  \frametitle{Mixture of Gaussians: Complete Data Likelihood}
  If we know the mixture assignments $z_i$ for each sample, we could write out
  the likelihood very explicitly.
  \begin{align*}
\log p\left(x, z\right) &= \log \left[\prod_{i = 1}^{n} \prod_{k = 1}^{K} \left(p_{k}\Gsn\left(x_i \vert \mu_k, \Sigma_k\right)^{\indic{z_i = k}}\right)\right] \\
&= \sum_{i = 1}^{n} \sum_{k = 1}^{K} \indic{z_i = k} \left[\log p_{k} + \log \Gsn\left(x_i \vert \mu_k, \Sigma_k\right)\right] \\
&= \sum_{i = 1}^{n} \sum_{k = 1}^{K} \indic{z_i = k} \left[\log p_{k} -\frac{D}{2}\log\left(2\pi\right) - \frac{1}{2}\log \left|\Sigma_k\right| -  \\ &\qquad\frac{1}{2} \left(x_i - \mu_k\right)^{T}\Sigma^{-1}_{k} \left(x_i - \mu_k)\right]
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Marginalizing $z$}
  \begin{itemize}
  \item Can't use the previous derivation if we don't know the specific
    configuration of $z$
  \item Way too many configurations to actual do this sum
  \end{itemize} 
  \begin{align*}
    \log p\left(x\right) &= \log \sum_{z} p\left(x, z\right)
  \end{align*} 
\end{frame}

\begin{frame}
  \frametitle{Evidence Lower Bound}
  \begin{itemize}
  \item It's counterintuitive, but one problem-solving strategy is to deliberately introduce complexity
  \item Complexity $\rightarrow$ more degrees of freedom
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Variational $q$}
  \begin{itemize}
  \item Consider some $q\left(z \vert x\right) \in \mathcal{Q}$, some large
    family of tractable densities
  \end{itemize} 
  \begin{align*}
    \log p\left(x\right) &= \Esubarg{q}{\log p\left(x\right)} \\
    &= \Esubarg{q}{\log \frac{p\left(x, z\right)}{p\left(z \vert x\right)}} \\
    &= \Esubarg{q}{\log \frac{p\left(x, z\right)}{q\left(z \vert x\right)} \frac{q\left(z \vert x\right)}{p\left(z \vert x\right)}} \\
    &= \Esubarg{q}{\log p\left(x \vert z\right)} - D_{KL}\left(q\left(z \vert x\right) \vert \vert p\left(z\right)\right) + D_{KL}\left(q\left(z \vert x\right) \vert \vert p\left(z \vert x\right)\right)
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Studying the bound}
  \begin{align*}
    \log p\left(x\right) &= \Esubarg{q}{\log p\left(x \vert z\right)} - D_{KL}\left(q\left(z \vert x\right) \vert \vert p\left(z\right)\right) + D_{KL}\left(q\left(z \vert x\right) \vert \vert p\left(z \vert x\right)\right) \\
     &\geq \Esubarg{q}{\log p\left(x \vert z\right)} - D_{KL}\left(q\left(z \vert x\right) \vert \vert p\left(z\right)\right)
  \end{align*}
  \begin{itemize}
  \item Reconstruction error: How plausible is the observed $x$, averaging over
    assignments $z$, and considering the current likelihood estimate?
  \item Approximation complexity: How far is the approximating $q\left(z \vert
    x\right)$ from the posterior?
  \item How different is the posterior approximation $p\left(z \vert x\right)$
    from the actual posterior?
    \begin{itemize}
    \item $D_{KL} \geq 0$ for any pair of probabilities
    \item Hard to compute, so just drop and turn into an inequality
    \end{itemize}
  \end{itemize}
\end{frame}

\section{Variational Inference}
\label{sec:introduction}

\begin{frame}
  \frametitle{The Variational Idea}
  \begin{itemize}
  \item Transform integration problem into an optimization one
  \item Some families $\mathcal{Q}$ are easier to optimize over than others
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Doing Variational Inference}
  To practically implement this idea need to determine
  \begin{itemize}
  \item What densities $\mathcal{Q}$ can we actually work with?
  \item How are we going to measure the quality of an approximation?
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Doing Variational Inference}
  To practically implement this idea need to determine
  \begin{itemize}
  \item What densities $\mathcal{Q}$ can we actually work with?
    \begin{itemize}
    \item Mean-Field, Structured Mean-Field approximations, or even implicit densities
    \end{itemize}
  \item How are we going to measure the quality of an approximation?
    \begin{itemize}
    \item ELBO, $f$-divergences, Wasserstein distance, ...
    \end{itemize}
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Doing Variational Inference}
  To practically implement this idea need to determine
  \begin{itemize}
  \item What densities $\mathcal{Q}$ can we actually work with?
    \begin{itemize}
    \item \textbf{Mean-Field}, Structured Mean-Field approximations, or even implicit densities
    \end{itemize}
  \item How are we going to measure the quality of an approximation?
    \begin{itemize}
    \item \textbf{ELBO}, $f$-divergences, Wasserstein distance, ...
    \end{itemize}
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Mean-Field Approximation}
  Consider a family where all the variables factor
  \begin{align*}
    q\left(z_1, \dots, z_n\right) = \prod_{i = 1}^{n} q_{i}\left(z_i\right)
  \end{align*} 
  (notice that we drop conditioning on $x_$, this is actually more general than
  before)
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/mean_field_geometry}
  \caption{\label{fig:mean_field_geometry} }
\end{figure}
\end{frame}


\begin{frame}
  \frametitle{Optimization}
  \begin{itemize}
  \item Fixing all but the $i^{th}$ coordinate's distribution, can find how to
    optimize ELBO directly
  \end{itemize} 
  \begin{align*}
q_i\left(z_i\right) \propto \exp{\Esubarg{q_{-i}}{\log p\left(x, z_i, z_{-i}\right)}}
  \end{align*} 
\end{frame}

%% Maybe put some examples of this update here
%% mention that this is a lot of algebra, every time you want a model

\begin{frame}
  The proof of this fact is only a few lines (not at all obvious though)
\begin{align*}
\log p\left(x\right) &\geq
\Esubarg{q}{\log p\left(x \vert z\right)} - D_{KL}\left( q\left(z\right) \vert\vert p\left(z\right)\right) \\
&= \Esubarg{q}{\log p\left(x, z\right)} - \Esubarg{q}{\log q\left(z\right)}\\
&= \Esubarg{q_i}{\Esubarg{q_{-i \vert i}}{\log p\left(x, z_i, z_{-i})}} - \sum \Esubarg{q_j}{\log q_j\left(z_j\right)} \\
&\stackrel{c}{=} -D_{KL}\left(q_{i}\left(z_i\right) \vert \vert \exp{\Esubarg{q_{-i}}{p\left(x, z_i, z_{-i}\right)}}\right)
\end{align*}
Since KL is always $\geq 0$, can maximize this expression by setting it to zero
(set $q_i$ to right hand side distribution)
\end{frame}

\section{Variational Autoencoders}

\begin{frame}
  \frametitle{A Calculated Tradeoff}
  \begin{itemize}
  \item It's nice that this approach works for arbitrary mean-field $\prod
    q_{i}\left(z_i\right)$
  \item But it's restrictive to have to compute
    \begin{align*}
      q_i\left(z_i\right) \propto \exp{\Esubarg{q_{-i}}{\log p\left(x, z_i, z_{-i}\right)}}
    \end{align*}
    in closed form (not to mention tedious)
    \begin{itemize}
    \item How could we have (say) $p\left(x \vert z\right) = \Gsn\left(x \vert
      f_{\theta}\left(z\right), \exp{g_{\theta}\left(x\right)}\right)$ for some
      complicated $f, g$?
    \end{itemize} 
  \item Idea: Restrict family $\mathcal{Q}$ (hopefully not too much), and make
    optimization more automatic
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Inference Networks}
  \begin{itemize}
  \item New approximating family: $q_{\varphi}\left(z \vert x\right) = \Gsn\left(z \vert
    \mu_{\varphi}\left(x\right), \sigma^{2}_{\varphi}\left(x\right) I\right)$
  \item Let $\mu_{\varphi}$ and $\sigma^{2}_{\varphi}$ be arbitrary nonlinear
    functions
    \begin{figure}[
        \centering
        \includegraphics[width=0.7\paperwidth]{figure/vae_inference}
        \caption{\label{fig:vae_inference} }
    \end{figure}
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Optimization}
\begin{itemize}
\item Instead of optimizing one coordinate at a time, optimize all at once
  through $\varphi$
\item Can also optimize $\theta$ in the likelihood
\end{itemize}  
\end{frame}

\begin{frame}
  \frametitle{Reparametrization Trick}
  \begin{itemize}
  \item Taking gradient step along
  \begin{align*}
    \nabla_{\varphi} \Esubarg{q}{\log p_{\theta}\left(x \vert z\right)}
  \end{align*}
  is complicated, because

  \begin{align*}
    \nabla_{\varphi} \Esubarg{q}{\log p_{\theta}\left(x \vert z\right)} &= \int \log p_{\theta}\left(x \vert z\right)
    \nabla_{\varphi}q_{\varphi}\left(z \vert x\right)dz
  \end{align*}
  is no longer an expectation over $q_{\varphi}$.
  \item Can't just sample, and can't do integral analytically.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Reparametrization Trick}
  \begin{itemize}
  \item Sampling
  \begin{align*}
    z \vert x \sim q_{\varphi}\left(z \vert x\right)
  \end{align*}
  is sometimes the same as
  \begin{align*}
    \eps &\sim p_{0}\left(\eps\right) \\
    z \vert \eps, x &\equiv g_{\varphi}\left(x, \eps\right)
  \end{align*}
  for some deterministic $g_{\varphi}$
  \item Large family of densities $\rightarrow$ one density with large family of
    transformations
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Reparametrization Trick}
  Most common example, if you want to sample
  \begin{align*}
    z \vert x &\sim \Gsn\left(z \vert \mu_{\varphi}\left(x\right), \sigma_{\varphi}^{2}\left(x\right)\right)
  \end{align*} 
  instead use
  \begin{align*}
    \eps &\sim \Gsn\left(0, I\right) \\
    z \vert x, \eps &\equiv \mu_{\varphi}\left(x\right) + \sigma_{\varphi}^{2}\left(x\right) \odot \eps
  \end{align*}
\end{frame}

\begin{frame}
  \frametitle{Optimization after Reparameterization}
  \begin{itemize}
  \item Since $g_{\varphi}$ is deterministic, gradient can be easily
    approximated,
  \begin{align*}
    \nabla_{\varphi} \Esubarg{q_{\varphi}}{\log p_{\theta}\left(x \vert z\right)} &=
    \nabla_{\varphi} \Esubarg{p\left(\eps\right)}{\log p_{\theta}\left(x \vert g_{\varphi}\left(\eps\right)\right)} \\
    &= \Esubarg{p\left(\eps\right)}{\nabla \log p_{\theta}\left(x \vert g_{\varphi}\left(\eps\right)\right)} \\
      &\approx \sum_{i} \nalba \log_{\theta}p_{\theta}\left(x \vert g_{\varphi}\left(\eps_{i}\right)\right)
  \end{align*}
  after sampling lots of $\eps_{i} \sim p\left(\eps\right)$.
  \item We can optimize the ELBO!
  \end{itemize}
\end{frame}

\section{Applications}
\label{sec:applications}

\begin{frame}
  \frametitle{Latent Space & Generation}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/}
  \caption{\label{fig:} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Latent Space & Generation}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/vae_conditional}
  \caption{\label{fig:vae_conditional} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Probabilistic Tumor Segmentation}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/vae_unet}
  \caption{\label{fig:vae_unet} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Drug Design & Discovery}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/vae_molecule}
  \caption{\label{fig:vae_molecule} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{One-Shot Generalization}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/vae_omniglot}
  \caption{\label{fig:vae_omniglot} }
\end{figure}
\end{frame}

\end{document}
