\documentclass[10pt,mathserif]{beamer}

\input{preamble.tex}
\usepackage{graphicx,amsmath,amssymb,natbib}

%% formatting
\setbeamertemplate{navigation symbols}{}
\usecolortheme[rgb={0.13,0.28,0.59}]{structure}
\setbeamertemplate{itemize subitem}{--}
\setbeamertemplate{frametitle} {
	\begin{center}
	  {\large\bf \insertframetitle}
	\end{center}
}

\AtBeginSection[] 
{ 
	\begin{frame}<beamer> 
		\frametitle{Outline} 
		\tableofcontents[currentsection,currentsubsection] 
	\end{frame} 
} 

\title{\large \bfseries Metalearning}

\author{Kris Sankaran\\[3ex]
Nepal Winter School in AI}

\date{\today}

\begin{document}

\frame{
  \thispagestyle{empty}
  \titlepage
}

\section{Overview}
\begin{frame}
\frametitle{Learning Objectives}
\begin{itemize}\itemsep=12pt
\item Understand basic metalearning setup
\item Recognize metalearning problems ``in the wild''
\item Understand foundational algorithms, on which the rest the
  field stands
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Challenge}
  \begin{itemize}\itemsep=12pt
  \item Deep learning methods need lots of data
  \item Humans don't need a million examples of Yaks to be able to recognize
    a new one
  \item How can we have our machines learn from fewer examples?
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.4\paperwidth]{figure/yak}
  \end{figure}
\end{frame}


\begin{frame}
  \frametitle{Transfer Learning}
  \begin{itemize}\itemsep=12pt
  \item People figured out a while ago that you can \textit{transfer} to small
    data settings
  \item Train deep model one big dataset, then fine tune the top layers
  \item Bottom layers learn generic visual features
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.6\paperwidth]{figure/transfer_drawing}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Connection to Real-World}
  \begin{itemize}
  \item Different users of an app
  \item Different hospital databases
  \item Pest detection across types of crops
  \item Satellite imagery analysis across diverse environments
  \item ``Training Medical Image Analysis Systems like Radiologists'' \citep{maicas2018training}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Transfer Learning}
  \begin{itemize}\itemsep=12pt
  \item People figured out a while ago that you can \textit{transfer} to small
    data settings
  \item Train deep model one big dataset, then fine tune the top layers
  \item Bottom layers learn generic visual features
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.4\paperwidth]{figure/transfer_annotated}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Transfer Learning}
  \begin{itemize}\itemsep=12pt
  \item People figured out a while ago that you can \textit{transfer} to small
    data settings
  \item Train deep model one big dataset, then fine tune top layers to scarce
    samples
  \item Bottom layers learn generic visual features
  \end{itemize}
  \begin{figure}
    \includegraphics[width=0.8\paperwidth]{figure/transfer.png}
    \caption{From the pytorch transfer learning tutorial}
  \end{figure}
\end{frame}

\begin{frame}
  \begin{itemize}\itemsep=12pt
  \item The same underlying features seem useful across a lot of tasks...
  \item Transfer learning is nice, but requires lots of hand-tuning
  \item Can we learn models that automatically adapt in scarce data settings?
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metalearning Setup}
  \begin{itemize}\itemsep=12pt
    \item Forget about solutions for now, how should we formulate the problem?
    \item Create many small train / test datasets (calls these ``episodes'')
    \item Same classes don't have to appear in each episode
    \item Metalearner maps new datasets to new (hopefully adapted) algorithm
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Metalearning Setup}
  \begin{itemize}\itemsep=12pt
    \item Metalearner maps new datasets to new (hopefully adapted) algorithm
    \item Usual training / testing is like
      \begin{itemize}
        \item $D_{\text{train}} = \left(x_i, y_i\right)_{i = 1}^{n}$
        \item $D_{\text{test}} = \left(x_i, y_i\right)_{i = 1}^{n^\prime}$
      \end{itemize}
    \item New training / testing is like
      \begin{itemize}
      \item $D_{\text{metatrain}} = \left\{D_{\text{train}}^{n}, D_{\text{test}}^{n}\right\}_{n = 1}^{N}$
      \item $D_{\text{metatest}} = \left\{D_{\text{train}}^{n}, D_{\text{test}}^{n}\right\}_{n = 1}^{N^\prime}$
      \end{itemize}
    \item You don't have to fine-tune to each training / testing episode!
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Metalearning Setup (picture)}
  \begin{itemize}
  \item Given a new user, domain, etc $\rightarrow$ return a new algorithm
  \item Hope that you can share information across users / domains
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\paperwidth]{figure/metalearning_setup_boxes}
    \caption{Given a new training dataset $D_n$, the metalearner $\mathcal{A}$
      returns an adapted classifier
      $f_{\theta}$. \label{fig:metalearning_setup_boxes}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Metalearning Setup (picture)}
  \begin{itemize}
  \item Given a new user, domain, etc $\rightarrow$ return a new algorithm
  \item Hope that you can share information across users / domains
  \end{itemize}
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\paperwidth]{figure/metalearning_setup_curves}
    \caption{Given a new training dataset $D_n$, the metalearner $\mathcal{A}$
      returns an adapted classifier
      $f_{\theta}$. \label{fig:metalearning_setup_boxes}}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Discussion}
  Brainstorm some problems or application areas where metalearning might
  be useful?
\end{frame}

\begin{frame}
  \frametitle{How to do this?}
  \begin{itemize}
  \item \textbf{Idea 1}: Draw inspiration from existing algorithms that can
    adapt to new classes
  \item \textbf{Idea 2}: Learn a global model that can be ``perturbed'' to work
    in many different domains
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{How to do this?}
  \begin{itemize}
  \item \textbf{Idea 1}: Draw inspiration from existing algorithms that can
    adapt to new classes
    \begin{itemize}
    \item Nearest Neighbors
    \item Prototype methods
    \end{itemize}
  \item \textbf{Idea 2}: Learn a global model that can be ``perturbed'' to work
    in many different domains
    \begin{itemize}
    \item Like hierarchical / empirical bayesian models
    \end{itemize}
  \end{itemize}
\end{frame}

\section{Extending Existing Algorithms}

\begin{frame}
  \frametitle{Nearest Neighbors}
  \begin{itemize}
  \item Nearest neighbors knows how to handle new classes (e.g., Yaks)
  \item Key is that it has a distance between all examples
  \end{itemize} 
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.40\paperwidth]{figure/nneighbor_1}
    \caption{The original data, just two classes. \label{fig:nneighbor_1} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Nearest Neighbors}
 \begin{itemize}
 \item Nearest neighbors knows how to handle new classes (e.g., Yaks)
 \item Key is that it has a distance between all examples
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.40\paperwidth]{figure/nneighbor_2}
  \caption{Can draw the decision boundary between two classes. \label{fig:nneighbor_2} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Nearest Neighbors}
 \begin{itemize}
 \item Nearest neighbors knows how to handle new classes (e.g., Yaks)
 \item Key is that it has a distance between all examples
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.40\paperwidth]{figure/nneighbor_3}
  \caption{What if we see a third class? \label{fig:nneighbor_3} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Nearest Neighbors}
  \begin{itemize}
  \item Nearest neighbors knows how to handle new classes (e.g., yaks)
  \item Key is that it has a distance between all examples
  \end{itemize} 
  \begin{figure}[ht]
    \centering
    \includegraphics[width=0.40\paperwidth]{figure/nneighbor_4}
    \caption{Just introduce new decision boundaries. \label{fig:nneighbor_4} }
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Nearest Neighbors (Sharing Information)}
 \begin{itemize}
 \item If we run nearest neighbors separately on each task, we aren't sharing
   any information
 \item We want to use information we learned from data with horses to be able to
   classify yaks.
 \item Idea: Learn a good cross-task representation.
 \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Smoothing Nearest Neighbors}
 \begin{itemize}
 \item To learn shared representation, we to use deep learning
 \item But nearest neighbors is not differentiable! Can't use backprop.
 \item Idea: Smooth nearest neighbors
 \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Smoothing Nearest Neighbors}
 \begin{itemize}
 \item Idea: Smooth nearest neighbors
 \end{itemize} 
 \begin{align}
   \hat{y}\left(x\right) &= y_i \indic{N\left(x\right) = x_i} \\
   &= \sum_{j = 1}^{n} y_j a\left(x, x_j\right)
 \end{align} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.65\paperwidth]{figure/hard_a_fun}
  \caption{Nearest neighbors bases its prediction entirely on the nearest
    neighbor. \label{fig:hard_a_fun} }
\end{figure}

\end{frame}

\begin{frame}
  \frametitle{Smoothing Nearest Neighbors}
 \begin{itemize}
 \item Idea: Smooth nearest neighbors
 \end{itemize} 
 \begin{align}
   \hat{y}\left(x\right) &\approx \sum_{j = 1}^{n} y_j \tilde{a}\left(x, x_j\right) \\
   \tilde{a}\left(x, x_i\right) &:= \frac{\exp{-d\left(x, x_i\right)}}{\sum_{j = 1}^{n} \exp{-d\left(x, x_j\right)}}
 \end{align} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.65\paperwidth]{figure/soft_a_fun}
  \caption{We can smooth this out by allowing contributions that decay with
    distance. \label{fig:soft_a_fun} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Discussion}
  \begin{itemize}
  \item For $k$-Nearest Neighbors, larger $k$ reduces variance but increases bias
  \item $k$ controls model complexity
  \item How do we control complexity in this algorithm?
  \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Shared Embeddings}
 \begin{itemize}
 \item How to share representations across tasks? 
 \item Learn shared embedding functions: $x \rightarrow g_\varphi\left(x\right)$
 \end{itemize} 
\end{frame}

\begin{frame}
  \frametitle{Overall Process \citep{vinyals2016matching}}
 \begin{itemize}
 \item  Across all the tasks, learn a common embedding
 \item Task-specific nearest neighbors classifiers (since task-specific classes)
 \item Since function $a\left(x, x_i\right)$ is smooth, can backpropagate errors
   to learn optimal $g$
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\paperwidth]{figure/nn_full_process}
  \caption{The overall process, from datasets, to shared embedding, to learned
    classifiers. \label{fig:nn_full_process} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes \citep{snell2017prototypical}}
 \begin{itemize}
 \item An alternative to nearest neighbors that also works with new classes is
   the prototype method
 \item Define prototypes for each class, and assign new examples to the closest
   prototype
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_1}
  \caption{The original two class dataset.\label{fig:prototypes_1} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes}
 \begin{itemize}
 \item Prototypes: $c_k = \frac{1}{N_k} \sum_{i : y_i = k} x_i$
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_2}
  \caption{Define prototypes for each class.\label{fig:prototypes_2} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes}
 \begin{itemize}
 \item $\hat{y}\left(x\right) = \arg\min_{k \in \{\text{green, purple}\}} d\left(x, c_k\right)$
 \item If you want probabilities, $\Parg{y\left(x\right) = k \vert D^{n}} \propto \exp{-d\left(x, c_k \right)}$
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_3}
  \caption{Predictions are made according to distance to the prototypes.\label{fig:prototypes_3} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes}
 \begin{itemize}
   \item Now we see $\left(x_i, y_i = \text{blue}\right)$
 \item This blue class may have never appeared in any of our metatraining
   examples
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_4}
  \caption{We can introduce a new class.\label{fig:prototypes_4} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes}
 \begin{itemize}
 \item $\hat{y}\left(x\right) = \arg\min_{k \in \{\text{green, purple, blue}\}} d\left(x, c_k\right)$
 \item If you want probabilities, $\Parg{y\left(x\right) = k \vert D^n} \propto \exp{-d\left(x, c_k \right)}$
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_5}
  \caption{The new datapoint becomes a prototype, and we make classifications
    according to distance to prototypes, as before.\label{fig:prototypes_5} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Prototypes: Sharing across tasks}
 \begin{itemize}
 \item Sharing accomplshed through a common embedding $g_{\varphi}\left(x\right)$
 \item Prototypes: $c_k = \frac{1}{N_k} \sum_{i : y_i = k} f_{\varphi}\left(x_i\right)$
 \item $\hat{y}\left(x\right) = \arg \min_{k} d\left(f_\varphi\left(x\right), c_k\right)$
 \item $\Parg{\hat{y}\left(x\right) = k \vert D^{n}} \propto \exp{-d\left(f_\varphi\left(x\right), c_k\right)}$
 \end{itemize} 
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.35\paperwidth]{figure/prototypes_5}
  \caption{To share across tasks, we learn prototypes after embedding all points
    according to some shared embedding function \label{fig:prototypes_5}.}
\end{figure}
\end{frame}

\section{Perturbation Based}

\begin{frame}
  \frametitle{Model Agnostic Meta-Learning \citep{finn2017model}}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.4\paperwidth]{figure/maml_global}
  \caption{On new tasks, adapt the global parameter.\label{fig:maml_global} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Learning Strategy}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/maml_learning_1}
  \caption{Each task has it's own loss landscape over
    $\Theta$. \label{fig:maml_learning_1} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Learning Strategy}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
   \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/maml_learning_0}
  \caption{We start with some guess at a global
    $\theta$. \label{fig:maml_learning_2} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Learning Strategy}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
   \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/maml_learning_2}
  \caption{We find task-specific $\theta_i$'s by taking a step from this global
    $\theta$. \label{fig:maml_learning_3} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Learning Strategy}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.6\paperwidth]{figure/maml_learning_3}
  \caption{We update the global $\theta$ according to the averarge direction across all the tasks.
    \label{fig:maml_learning_4} }
\end{figure}
\end{frame}

\begin{frame}
  \frametitle{Learning Strategy}
 \begin{itemize}
 \item Imagine that the model parameters will be similar across tasks
 \item Learn a global parameter and adapt it on a task-by-task basis
 \item (This is analogous to transfer learning)
 \end{itemize}
\begin{figure}[ht]
  \centering
  \includegraphics[width=0.5\paperwidth]{figure/maml_paper}
  \caption{In case you are interested in the details, check against the original
    paper
    \label{fig:maml_paper} }
\end{figure}
\end{frame}

\begin{frame}[allowframebreaks]
  \bibliographystyle{plain}
    \bibliography{refs}
\end{frame}

\end{document}
